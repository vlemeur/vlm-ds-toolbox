{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lists usefull Data sources : \n",
    "\n",
    "- UCI Machine Learning repository : https://archive.ics.uci.edu/ml/index.php\n",
    "- UEA & UCR Time Series Classification Repository : https://timeseriesclassification.com/dataset.php"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usefull tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Downsampling and Upsampling can be usefull to adapt data\n",
    "- Using Interpolation technique carefully\n",
    "- Smoothing can be usefull to use data for prediction (ex: pd.ewm performs exponential weighted smoothing : recent data have greater weight)\n",
    "- Carefull know what the timestamps used mean (upload or data acquisition..) not to add lookahead in data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Use Histogram, plots\n",
    "- Statistical summary tables \n",
    "- Correlation tables \n",
    "- plot with diff to remove trends and time correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from tsfresh import extract_features, extract_relevant_features\n",
    "from tsfresh.examples.robot_execution_failures import load_robot_execution_failures, download_robot_execution_failures\n",
    "from statsmodels.graphics.tsaplots import plot_pacf, plot_acf\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.varmax import VARMAX\n",
    "from pmdarima.arima import auto_arima\n",
    "import statsmodels.api as sm\n",
    "from pandas import Series\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "download_robot_execution_failures()\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import matplotlib.pyplot as plt \n",
    "%pylab inline\n",
    "plt.style.use('dark_background')\n",
    "import plotly.io as pio\n",
    "from ds_toolbox.graphs import plot_evolution, plot_hist, plot_xy\n",
    "plt.figure(figsize(20,10))\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "pio.templates.default = 'plotly_dark'\n",
    "plt.style.use(\"dark_background\")\n",
    "dg = pd.read_csv('data_general/Daily_Demand_Forecasting_Orders.csv', sep=';')\n",
    "# Data Daily total female births in California, 1959\n",
    "df = pd.read_csv('data_general/daily-total-female-births-in-cal.csv', sep=',').iloc[0:365]\n",
    "df.columns = ['date', 'female_births']\n",
    "df.index= pd.to_datetime(df['date'])\n",
    "df = plot_hist(df=df, keys=['female_births'], kernel_density='gaussian')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stationarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A time series is stationarity if for any lags the distribution of values is equal <=> Strong Stationarity\n",
    "\n",
    "Possible to transform data to make them stationarity (by differenciation, logarithm, squared) : but important to keep in mind the meaning of such transformation about data Informations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df['moving_mean'] = df.expanding(min_periods=2).mean()\n",
    "df['moving_std'] = df['female_births'].expanding(min_periods=2).std()\n",
    "df['EMA'] = df['female_births'].ewm(span=40,adjust=False).mean()\n",
    "df = plot_evolution(df=df, keys=['female_births', 'moving_mean', 'moving_std', 'EMA'], title='Female Births in California 1959')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print('Dickey-Fuller criteria: p=', str(sm.tsa.stattools.adfuller(df['female_births'])[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df['auto_corr'] = [df['female_births'].autocorr(lag=i) for i in range(len(df))]\n",
    "df=plot_evolution(df=df.iloc[:len(df) -10], keys=['auto_corr'], title='Auto correlation functions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize(30,10))\n",
    "plot = plot_pacf(df['female_births'], lags=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spirious Correlation vs Cointegration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Spirous Correlation are very high value of correlation without any reason (it could happend when there is a huge trend)\n",
    "- Cointegration is high correlation due to real relation. Ex: Drunk Pedestrian with dog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual Seasonal Trend Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize(20,10))\n",
    "plot = sm.tsa.seasonal_decompose(df['female_births']).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Models for Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordinary least squares linear regression can be applied to time series data with following assumuptions : \n",
    "\n",
    "1. Assumptions with respect to behavior of time series\n",
    "- time series has a linear response to its predictors (might be avoid this using polynomial features)\n",
    "- No input variable is constant over time or perfectly correlated with another input variable : Independent variables to account for the temporal dimension of the data\n",
    "\n",
    "2. Assumptions with respect to the error\n",
    "- For each point in time the expected value of the error given all explanatory variables for all time periods is 0 : closed to impossible in practice \n",
    "- Error at any given time period is uncorrelated with inputs at any time periof in the past or future.\n",
    "- Variance of error is independant of time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-Regressive Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuition : Past predicts the future\n",
    "$$y_t = b_0 + b_1 \\times y_{t-1} + e _t$$\n",
    "$e_t$ : variable term with mean at 0 and constant variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AR(p) model Description : \n",
    "$$ y_t = b_0 + b_1 \\times y_{t-1}  + b_2 \\times y_{t-2}  + b_3 \\times y_{t-3} + ... +  b_p \\times y_{t-p} +  e _t $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex : Ar(1)\n",
    "$$ E(y_t) = \\mu = \\frac{b_0}{1-b_1}$$\n",
    "$$ var(y_t) = \\frac{var(e_t)}{1-b_1^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which value for p ?\n",
    "\n",
    "1. Use of Partial Autocorellation function and detect the first lag where it mets 5% threshold\n",
    "2. Use of Akaike information Criterion : \n",
    "$$ AIC = 2k - 2lnL$$\n",
    "$k$ : number of parameters\n",
    "$L$ : likelihood of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dljung = plot_evolution(df=acorr_ljungbox(df['female_births'].values, model_df=1, return_df=True), keys=['lb_pvalue'], title='Result of Ljung-Box for differents lags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dg = plot_evolution(df=dg, keys=['Target (Total orders)'], x_axis_name='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize(30,10))\n",
    "\n",
    "plot = plot_pacf(dg['Target (Total orders)'], lags=29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model1 = AutoReg(dg['Target (Total orders)'].values, lags=1).fit()\n",
    "model2 = AutoReg(dg['Target (Total orders)'].values, lags=2).fit()\n",
    "model3 = AutoReg(dg['Target (Total orders)'].values, lags=3).fit()\n",
    "model4 = AutoReg(dg['Target (Total orders)'].values, lags=4).fit()\n",
    "model5 = AutoReg(dg['Target (Total orders)'].values, lags=5).fit()\n",
    "\n",
    "dg['prediction_1'] = model1.predict(start=0, end=len(dg), dynamic=False)\n",
    "dg['prediction_2']=  [np.nan] + [item for item in model2.predict(start=0, end=len(dg), dynamic=False)]\n",
    "dg['prediction_3']=  [np.nan,np.nan] + [item for item in model3.predict(start=0, end=len(dg), dynamic=False)]\n",
    "dg['prediction_4']=  [np.nan,np.nan,np.nan] + [item for item in model4.predict(start=0, end=len(dg), dynamic=False)]\n",
    "dg['prediction_5']=  [np.nan,np.nan,np.nan,np.nan] + [item for item in model5.predict(start=0, end=len(dg), dynamic=False)]\n",
    "dg =  plot_evolution(df=dg, keys=['Target (Total orders)', 'prediction_1','prediction_2','prediction_3','prediction_4', 'prediction_5'], x_axis_name='index')\n",
    "# make prediction\n",
    "# yhat = model_fit.predict(dg['Target (Total orders)'].values[1:])\n",
    "# yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving Average Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of each point in time is a function of the recent past value \"error\" terms (each of which is independant from the others)\n",
    "\n",
    "$$ y_t = \\mu + e_t + \\theta_1 \\times e_{t-1} + \\theta_2 \\times e_{t-2} + ... + \\theta_q \\times e_{t-q}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concept : many independent events at different past times affect current value of the process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg['auto_corr'] = [dg['Banking orders (2)'].autocorr(lag=i) for i in range(len(dg))]\n",
    "#from ds_toolbox.graphs import add_horizontal_line_trace\n",
    "dg=plot_evolution(\n",
    "    df=dg, keys=['auto_corr'],\n",
    "    title='Auto correlation modes',\n",
    "    modes=['markers'],\n",
    "    bandwith={'up_value': 0.2, 'down_value':-0.2}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# MA example\n",
    "# fit model\n",
    "MA1 = ARIMA(dg['Banking orders (2)'], order=(0,0, 1)).fit()\n",
    "MA2 = ARIMA(dg['Banking orders (2)'], order=(0,0, 2)).fit()\n",
    "# make prediction\n",
    "dg['prediction_ma1']=  [item for item in MA1.predict(start=0, end=len(dg) - 1, dynamic=False)]\n",
    "dg['prediction_ma2']=  [item for item in MA2.predict(start=0, end=len(dg) - 1, dynamic=False)]\n",
    "dg = plot_evolution(df=dg, keys=['Banking orders (2)', 'prediction_ma1', 'prediction_ma2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoregressive Integrated Moving Average Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combination of AM and MA models with acount of differencing to remove trends and rendering a time series stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ y_t = \\phi_1 \\times y_{t-1} + ... + \\phi_p \\times y_{t-p} + e_t +  \\theta_1 \\times e_{t-1}+ ... + \\theta_q \\times e_{t-q}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'I' term in ARIMA stands for integrated which refers to how many times the modeled time series must be differenced to produce stationarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_arima_model(X, arima_order):\n",
    "    # prepare training dataset\n",
    "    train_size = int(len(X) * 0.66)\n",
    "    train, test = X[0:train_size], X[train_size:]\n",
    "    history = [x for x in train]\n",
    "    # make predictions\n",
    "    predictions = list()\n",
    "    for t in range(len(test)):\n",
    "        model = ARIMA(history, order=arima_order)\n",
    "        model_fit = model.fit(disp=0)\n",
    "        yhat = model_fit.forecast()[0]\n",
    "        predictions.append(yhat)\n",
    "        history.append(test[t])\n",
    "    # calculate out of sample error\n",
    "    try:\n",
    "        error = mean_squared_error(test, predictions)\n",
    "    except:\n",
    "        error = np.inf\n",
    "    return error\n",
    "\n",
    "def evaluate_models(dataset, p_values, d_values, q_values):\n",
    "    dataset = dataset.astype('float32')\n",
    "    best_score, best_cfg = float(\"inf\"), None\n",
    "    for p in p_values:\n",
    "        for d in d_values:\n",
    "            for q in q_values:\n",
    "                order = (p,d,q)\n",
    "                try:\n",
    "                    mse = evaluate_arima_model(dataset, order)\n",
    "                    if mse < best_score:\n",
    "                        best_score, best_cfg = mse, order\n",
    "                    print('ARIMA%s MSE=%.3f' % (order,mse))\n",
    "                except:\n",
    "                    continue\n",
    "    print('Best ARIMA%s MSE=%.3f' % (best_cfg, best_score))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "evaluate_models(dg['Banking orders (2)'].values, [0,1,2,3,4,5], [0,1,2], [0,1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arima = auto_arima(\n",
    "    dg['Banking orders (2)'].values,\n",
    "    start_p=1, start_q=1,\n",
    "   max_p=5, max_q=5, m=12,\n",
    "   start_P=0, seasonal=False,\n",
    "   d=1, D=2, trace=True,\n",
    "   error_action='ignore',  \n",
    "   suppress_warnings=True, \n",
    "   stepwise=True\n",
    ")\n",
    "print(arima.aic())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dg['prediction_autoarima'] = arima.predict_in_sample()\n",
    "dg = plot_evolution(df=dg, keys=['Banking orders (2)', 'prediction_ma1', 'prediction_ma2', 'prediction_autoarima'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAR and VARMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a multivariate version of Autoregression : each variable has it's own auto regressive equation.\n",
    "In matrix notation : \n",
    "$$y_t = \\phi_0 + \\phi_1 * y_{t-1} + \\phi_2 * y_{t-2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "model1 = VARMAX(dg[['Target (Total orders)', 'Banking orders (2)']].values).fit()\n",
    "dg['target_varmax'] = [item[0] for item in model1.predict()]\n",
    "dg['banking_varmax'] =[item[1] for item in model1.predict()]\n",
    "dg = plot_evolution(df=dg, keys=['Banking orders (2)',  'banking_varmax'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seasonal ARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seasonal ARIMA (SARIMA) model assumes multiplicative seasonality. \n",
    "\n",
    "ARIMA (p,d,q) * (P,D,Q)m\n",
    "\n",
    "The model postulates that the seasonal behavior itself can be thought of as an ARIMA process with m specifying the number of time steps per seasonal cycle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usefull Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When trying to fit a AR model, it's usefull to perform  ACF on residual to see if we miss term. Performing it on series directly isn't efficient for AR model\n",
    "- Using Ljung-Box test can confirm previous results\n",
    "- ACF usefull to estimate all the interesting terms\n",
    "- Model selection : If ACF behavior Falls of slowly and if PACF sharp drop after lag = p => AR best guess IF ACF sharp drop after lag=q and fall off slowly then MA, if no sharp cutoff anywere : ARMA\n",
    "- General method : \n",
    "1. Determine wether AR MA or ARMA\n",
    "2. Test by increasing order and inspect residual : if large pacf then increase autoregressive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros and Cons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantages of Statistical Methods for Time Series : \n",
    "- models are simple and transparent\n",
    "- good if only small data available\n",
    "- practical methods to perform fit\n",
    "\n",
    "Disadvantages : \n",
    "- performance of these models don't realy improove with more data\n",
    "- Puts the focus on point estimates of the mean value of a distribution rather than on the distrbution.\n",
    "- Will do poor job describing data where nonlinear relationships are dominant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State Space Models for Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired from real-word usecases motivations. State space models osit a world in which the true state cannot be measured directly but only inferred frmo what can be measured. State space models also rely on specifying the dynamics of a system such as how the true state of the world evolves over time both due to internal dynamics and the external forces that are applied to a system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three main applications : \n",
    "\n",
    "- Filtering : way of deciding how to weigh the most recent information against past information in updating our estimate of state\n",
    "- Forecasting is the prediction of the future state without any information about the future.\n",
    "- Smoothing is the use of future and past information in making a best estimate of the state at a given time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kalman Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematical formulation : \n",
    "\n",
    "$$ x_t = F \\times x_{t-1} + B \\times u_{t} + w_{t} $$\n",
    "$$ y_t = A \\times x_t + v_t $$\n",
    "$$\\hat{x}_t = K_{t} \\times y_t + (1-K_t) \\times \\hat{x}_{t-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hiden Markov Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hidden Markov Model (HMM) is a statistical Markov model in which the system being modeled is assumed to be a Markov process – call it X  – with unobservable (\"hidden\") states. HMM assumes that there is another process Y  whose behavior \"depends\" on  X. The goal is to learn about X by observing Y  Y. HMM stipulates that, for each time instance n}, the conditional probability distribution of Y n given the history { X n = x n } n ≤ n0  must not depend on { x n } n < n 0 . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from hmmlearn import hmm\n",
    "\n",
    "##############################################################\n",
    "# Prepare parameters for a 4-components HMM\n",
    "# Initial population probability\n",
    "startprob = np.array([0.6, 0.3, 0.1, 0.0])\n",
    "# The transition matrix, note that there are no transitions possible\n",
    "# between component 1 and 3\n",
    "transmat = np.array([[0.7, 0.2, 0.0, 0.1],\n",
    "                     [0.3, 0.5, 0.2, 0.0],\n",
    "                     [0.0, 0.3, 0.5, 0.2],\n",
    "                     [0.2, 0.0, 0.2, 0.6]])\n",
    "# The means of each component\n",
    "means = np.array([[0.0,  0.0],\n",
    "                  [0.0, 11.0],\n",
    "                  [9.0, 10.0],\n",
    "                  [11.0, -1.0]])\n",
    "# The covariance of each component\n",
    "covars = .5 * np.tile(np.identity(2), (4, 1, 1))\n",
    "\n",
    "# Build an HMM instance and set parameters\n",
    "model = hmm.GaussianHMM(n_components=4, covariance_type=\"full\")\n",
    "\n",
    "# Instead of fitting it from the data, we directly set the estimated\n",
    "# parameters, the means and covariance of the components\n",
    "model.startprob_ = startprob\n",
    "model.transmat_ = transmat\n",
    "model.means_ = means\n",
    "model.covars_ = covars\n",
    "###############################################################\n",
    "\n",
    "# Generate samples\n",
    "X, Z = model.sample(500)\n",
    "\n",
    "# Plot the sampled data\n",
    "plt.plot(X[:, 0], X[:, 1], \".-\", label=\"observations\", ms=6,\n",
    "         mfc=\"orange\", alpha=0.7)\n",
    "\n",
    "# Indicate the component numbers\n",
    "for i, m in enumerate(means):\n",
    "    plt.text(m[0], m[1], 'Component %i' % (i + 1),\n",
    "             size=17, horizontalalignment='center',\n",
    "             bbox=dict(alpha=.7, facecolor='w'))\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating and Selecting Features for Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use of tsfresh python module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ts, y  = load_robot_execution_failures()\n",
    "df_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_features = extract_features(df_ts, column_id=\"id\", column_sort=\"time\")\n",
    "extracted_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_filtered_direct = extract_relevant_features(df_ts, y,column_id='id', column_sort='time')\n",
    "features_filtered_direct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning for Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pretty similar to cross-sectional Machine Learning\n",
    "- xgboost seems extremely effective with lightBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distances metrics used for Time Series : \n",
    "- Fréchet Distance : macimum distance between two curves during a time warping like traversal of the curves that always seeks to minimize the distance between two curves\n",
    "- Pearson correlation \n",
    "- Longest common subsesequence\n",
    "- Dynamic Time Warping (DTW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](data_general/images/Euclidean-distance-vs-DTW.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dstock = pd.read_csv('data_general/stocks.csv', sep=',', index_col='Date')\n",
    "dstock.index = pd.to_datetime(dstock.index)\n",
    "\n",
    "km_dba = TimeSeriesKMeans(\n",
    "    n_clusters=5,\n",
    "    metric=\"dtw\",\n",
    "    max_iter=5,\n",
    "    max_iter_barycenter=5,\n",
    "    random_state=0).fit(dstock.values)\n",
    "dstock['cluster'] = km_dba.predict(dstock.values)\n",
    "sns.countplot(x='cluster',data=dstock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(data=dstock, x=\"AAPL\", y=\"AMZN\",  hue='cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
